---
title: "Beer!"
author: "Jasmine Dumas"
date: "December 14, 2015"
output: word_document
---
1. **Data source**:
```{r}
beer = read.table("http://www.craftbeeranalytics.com/uploads/3/3/8/9/3389428/ratebeer_beerjobber.txt", header=TRUE, sep="")
save(beer, file="beer.RData")
head(beer)
colnames(beer) # we'd be predicting column score.overall using abv, ratings, & score.by.style
```

2. **Data Clean**:
```{r}
beer <- na.omit(beer) 
```

3. **Research Question**: What contributes to beer rating scores overall, abv, style, brewer?   
4. **Dependent Variable**: score.overall   
5. **Independent Variable**: score.by.style, name, brewer, style, abv, ratings    
6. **Quantitative vs. Qualitative**: The quantitative variables are score.overall, abv, ratings,score.by.style. The qualitative variable is name and brewer.  
7. **Correlation and Scatter plots**: We can try a linear model initially.
```{r}
# no multicollinearity between continuous variables - want to be low!
cor(beer$abv, beer$ratings)
cor(beer$abv, beer$score.by.style)
cor(beer$ratings, beer$score.by.style)

# want correlation between response and covariates - want to be high!
cor(beer$score.overall, beer$abv)
cor(beer$score.overall, beer$ratings)
cor(beer$score.overall, beer$score.by.style)

# what do these variables all look like against the response y variable
library(ggplot2)
ggplot(beer, aes(y = score.overall, x = abv)) + geom_point() # log?
ggplot(beer, aes(y = score.overall, x = ratings)) + geom_point() # log?
ggplot(beer, aes(y = score.overall, x = score.by.style)) + geom_point() # weird-linear?
# these graphs look weird maybe try inverse or logrithm scale and re-graph?
#source('~/Desktop/depaul/CSC423/multiplot.R', echo=TRUE)
#multiplot(a, b, c, cols=1)
```

8. **Building the model(s)**: The initial model has a high F-Statistic, significant p-value, and High R-squared/adj R-squared value which could be improved upon my removing the variable of brewer, which has many not significant p-values. 

After running the second model each of the co-variates deems to be significant (<0.05) except the abv value, the R-squared and Adjusted R-squared are lower than before but are still high (_at 95%, and 94% respectively which means that `beer.model1` can account for 95%, and 94% of the error present in the data)_, and the F-Statistic is High along with the significant p-value.       

The next 2 models seeks to fit the parsimonious attempts of simplifying the model while producing the aforementioned high F-Statistic, significant p-value, and High R-squared/adj R-squared values.

The automated step-wise regression algorithms verifies that the Lowest AIC score is achieved with the two co-variates of `score.by.style + style`

```{r}
# full model
beer.model = lm(score.overall ~ abv + ratings + score.by.style + brewer + style, data=beer) 
summary(beer.model) # High F-statistic, High R-squared/adj R-squared value

# remove brewer
beer.model1 = lm(score.overall ~ abv + ratings + score.by.style + style, data=beer) 
summary(beer.model1) # High F-statistic, High R-squared/adj R-squared value

# remove abv
beer.model2 = lm(score.overall ~ ratings + score.by.style + style, data=beer) 
summary(beer.model2) # better significant results

# remove ratings
beer.model3 = lm(score.overall ~ score.by.style + style, data=beer) 
summary(beer.model3) # better significant results

# automated selection algorithm using stepwise regression for the final 
full.beer = step(beer.model3, direction="both")
```

9. **Checking of assumptions:** A noticeable sign of slight multi-colinearity can be observed in checking the correlation between `cor(beer$ratings, beer$score.by.style)`

10. **Checking for interaction terms:** Without expansive domain experience in the craft beer industry and with the few amounts of co-variates I would not test or see the rationale behind interaction terms for this model.

11. **Checking for higher order models:** As noticed in the scatter plots, there are some intriguing distributions of the co-variates against the response variable, y. Slight curves could signal a second order model or a different transformation.

12. **Examination of residuals:** The residual plot takes a look at the observed values versus the predicted values from the final model. (Residual = Observed â€“ Predicted; or `e = y - y-hat`). There doesn't seem to be an extreme obvious trend in the dispersion of the plot, but it can be observed a slight curve possibly signaling a transformation is needed to improve the residuals.
```{r}
qplot(beer$score.overall, resid(beer.model3), geom="point") + labs(title="Final Model Residual Plot - beer.model3")
```

13. **Transformations:** After performing both log and inverse transformations of the response variable, the original `beer.model3` seems to be the most effective and has the highest R-squared value among the others.
```{r}
## log transformations of the y
score.overall = beer$score.overall
score.by.style = beer$score.by.style
style = beer$style

log.y = log(score.overall)
# http://stackoverflow.com/questions/8415778/lm-na-nan-inf-error
log.y[which(!is.finite(log.y))] = NA

beer.model4 = lm(log.y ~ score.by.style + style) 
summary(beer.model4) # less significant results / lower R-squared values
# to avoid getting a plot of differing lengths error i popped off the first row just in order to see
# the plot. This is an artifact from the previous -Inf error. Still a valid residual plot
qplot(log.y[-1], resid(beer.model4), geom="point") + labs(title="Final Model Residual Plot - beer.model4")


## inverse transformtions of the y
inv.y = 1 / score.overall
# http://stackoverflow.com/questions/8415778/lm-na-nan-inf-error
inv.y[which(!is.finite(inv.y))] = NA

beer.model5 = lm(inv.y ~ score.by.style + style) 
summary(beer.model5) # even less significant results than log / lower R-squared values
# to avoid getting a plot of differing lengths error i popped off the first row just in order to see
# the plot. This is an artifact from the previous -Inf error. Still a valid residual plot
qplot(inv.y[-1], resid(beer.model5), geom="point") + labs(title="Final Model Residual Plot - beer.model5")
```


14. **Conclusions:** The final model of y = $\beta_{0}$ + $\beta_{1}$ score.by.style + $\beta_{2}$ style seems to be with some degree of confidence a good predictor of the response y variable of overall score for this data set. For a more particular model it would have been nice to have additional attributes of the beer such as ingredients. 

