{
    "contents" : "---\ntitle: \"Beer!\"\nauthor: \"Jasmine Dumas\"\ndate: \"December 12, 2015\"\noutput: word_document\n---\n1. **Data source**:\n```{r}\nbeer = read.table(\"http://www.craftbeeranalytics.com/uploads/3/3/8/9/3389428/ratebeer_beerjobber.txt\", header=TRUE, sep=\"\")\nsave(beer, file=\"beer.RData\")\nhead(beer)\ncolnames(beer) # we'd be predicting column score.overall using abv, ratings, & score.by.style\n```\n\n2. **Data Clean**:\n```{r}\nbeer <- na.omit(beer) \n```\n\n3. **Research Question**: What contributes to beer rating scores overall, abv, style, brewer?   \n4. **Dependent Variable**: score.overall   \n5. **Independent Variable**: score.by.style, name, brewer, style, abv, ratings    \n6. **Quantitative vs. Qualitative**: The quantitative variables are score.overall, abv, ratings,score.by.style. The qualitative variable is name and brewer.  \n7. **Correlation and Scatter plots**: We can try a linear model initially.\n```{r}\n# no multicollinearity between continuous variables - want to be low!\ncor(beer$abv, beer$ratings)\ncor(beer$abv, beer$score.by.style)\ncor(beer$ratings, beer$score.by.style)\n\n# want correlation between response and covariates - want to be high!\ncor(beer$score.overall, beer$abv)\ncor(beer$score.overall, beer$ratings)\ncor(beer$score.overall, beer$score.by.style)\n\n# what do these variables all look like against the response y variable\nlibrary(ggplot2)\na=ggplot(beer, aes(y = score.overall, x = abv)) + geom_point() # log?\nb=ggplot(beer, aes(y = score.overall, x = ratings)) + geom_point() # log?\nc=ggplot(beer, aes(y = score.overall, x = score.by.style)) + geom_point() # weird-linear?\n# these graphs look weird maybe try inverse or logrithm scale and re-graph?\nsource('~/Desktop/depaul/CSC423/multiplot.R', echo=TRUE)\nmultiplot(a, b, c, cols=3)\n```\n\n8. **Building the model(s)**: The initial model has a high F-Statistic, significant p-value, and High R-squared/adj R-squared value which could be improved upon my removing the variable of brewer, which has many not significant p-values. \n\nAfter running the second model each of the co-variates deems to be significant (<0.05) except the abv value, the R-squared and Adjusted R-squared are lower than before but are still high (_at 95%, and 94% respectively which means that `beer.model1` can account for 95%, and 94% of the error present in the data)_, and the F-Statistic is High along with the significant p-value.       \n\nThe next 2 models seeks to fit the parsimonious attempts of simplifying the model while producing the aforementioned high F-Statistic, significant p-value, and High R-squared/adj R-squared values.\n\nThe automated step-wise regression algorithms verifies that the Lowest AIC score is achieved with the two co-variates of `score.by.style + style`\n\n```{r}\n# full model\nbeer.model = lm(score.overall ~ abv + ratings + score.by.style + brewer + style, data=beer) \nsummary(beer.model) # High F-statistic, High R-squared/adj R-squared value\n\n# remove brewer\nbeer.model1 = lm(score.overall ~ abv + ratings + score.by.style + style, data=beer) \nsummary(beer.model1) # High F-statistic, High R-squared/adj R-squared value\n\n# remove abv\nbeer.model2 = lm(score.overall ~ ratings + score.by.style + style, data=beer) \nsummary(beer.model2) # better significant results\n\n# remove ratings\nbeer.model3 = lm(score.overall ~ score.by.style + style, data=beer) \nsummary(beer.model3) # better significant results\n\n# automated selection algorithm using stepwise regression for the final \nfull.beer = step(beer.model3, direction=\"both\")\n```\n\n9. **Checking of assumptions:** A noticeable sign of slight multi-colinearity can be observed in checking the correlation between `cor(beer$ratings, beer$score.by.style)`\n\n10. **Checking for interaction terms:** Without expansive domain experience in the craft beer industry and with the few amounts of co-variates I would not test or see the rationale behind interaction terms for this model.\n\n11. **Checking for higher order models:** As noticed in the scatter plots, there are some intriguing distributions of the co-variates against the response variable, y. Slight curves could signal a second order model or a different transformation.\n\n12. **Examination of residuals:** The residual plot takes a look at the observed values versus the predicted values from the final model. (Residual = Observed â€“ Predicted; or `e = y - y-hat`). There doesn't seem to be an extreme obvious trend in the dispersion of the plot.\n```{r}\nplot(beer$score.overall, resid(beer.model3), main = \"Final Model Residual Plot - beer.model3\")\n```\n\n13. **Transformations if appropriate:** Check for distributions of co-variates in histograms/plots.\n```{r}\nhist(beer$abv)\nplot(beer$brewer)\nplot(beer$style)\nhist(beer$ratings)\nhist(beer$score.overall)\nhist(beer$score.by.style)\n```\n\n\n",
    "created" : 1449773538383.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1488419123",
    "id" : "E18278A3",
    "lastKnownWriteTime" : 1449854601,
    "path" : "~/Desktop/regression/beer.Rmd",
    "project_path" : "beer.Rmd",
    "properties" : {
        "ignored_words" : "variates,abv,multi,colinearity\n"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}