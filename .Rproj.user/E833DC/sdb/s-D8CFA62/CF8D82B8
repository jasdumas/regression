{
    "contents" : "---\ntitle: \"CSC 423 Homework - Chapter 6 & 7\"\nauthor: \"Jasmine Dumas\"\ndate: \"October 27, 2015\"\noutput: pdf_document\ngeometry: margin=0.5in\n---\n```{r, echo = FALSE}\nlibrary(knitr) # global settings\nlibrary(formatR)\nlibrary(rmarkdown)\nopts_chunk$set(tidy.opts=list(width.cutoff=60))\n```\n\npage 341 #6.8\n\n(a) Build a model for y, LOWBID and apply stepwise regression\n\n```{r}\nload(\"~/Desktop/depaul/CSC423/rdata/R/Exercises&Examples/FLAG2.Rdata\")\nhead(FLAG2)\nlibrary(MASS)\nFLAG2 <- na.omit(FLAG2) # step() requires removal of missing data before\nfull.road.model <- lm(LOWBID ~ DOTEST + LBERATIO + STATUS + DISTRICT + NUMBIDS + DAYSEST + RDLNGTH + PCTASPH + PCTBASE + PCTEXCAV + PCTMOBIL + PCTSTRUC + PCTTRAFF + SUBCONT, data = FLAG2)\nroad.model <- step(full.road.model, direction=\"both\")\nsummary(road.model)\n```\n\n(b) Interpret the $\\beta$'s\n\nFor every 1-unit increase in DOTEST would be multiplied by 9.095e-01  \nFor every 1-unit increase in LBERATIO would be multiplied by 8.297e+05  \nFor every 1-unit increase in NUMBIDS would be multiplied by -1.315e+04  \nFor every 1-unit increase in RDLNGTH would be multiplied by 6.601e+03  \nFor every 1-unit increase in PCTBASE would be multiplied by 3.104e+05  \nFor every 1-unit increase in PCTSTRUC would be multiplied by 3.351e+05  \nFor every 1-unit increase in PCTTRAFF would be multiplied by -6.652e+05\n\n(c) The dangers of drawing inferences from a stepwise model: A large number of t-test's have been conducted leading to a high probability of making one or more Type I or Type II errors being that we have included some unimportant independent variables in the the model, and second that we have eliminated some important independent variables.  Another danger is that we only tested a first order model/main effects and didn't include and higher order terms or interaction terms. We primarily use stepwise regression just to tell us which of the independent variable out of the masses are important to include into the model.\n\n\\pagebreak\npage 343 #6.9\n\n```{r}\n# Best Subset\nlibrary(leaps)\nyvar = c(\"LOWBID\")\nxvars = c(\"DOTEST\",\"LBERATIO\" ,\"STATUS\",\"DISTRICT\" ,\"NUMBIDS\", \"DAYSEST\" , \"RDLNGTH\" , \"PCTASPH\" , \"PCTBASE\",  \"PCTEXCAV\" ,\"PCTMOBIL\", \"PCTSTRUC\", \"PCTTRAFF\", \"SUBCONT\" )\nbest.model = leaps(x = FLAG2[,xvars], y=FLAG2[,yvar], names=xvars, nbest=3, method=\"adjr2\")\nbest.model$which # shows the T or F of variable inclusion in the model\nbest.model$adjr2\ndf = best.model$which\n```\n\nYes, as evident from row `which(df[19,])` the \"best subset\" model did select the same 7 variables as chosen in the stepwise regression method. `apply(df, 1, which)`\n\n```{r, echo = F, eval = T}\nwhich(df[19,])\n```\n\n\\pagebreak\npage 343 #6.10\n\n(a) Stepwise regression (with stepwise selection) to find the \"best predictors\" of heat rate (y)\n```{r}\nload(\"~/Desktop/depaul/CSC423/rdata/R/Exercises&Examples/GASTURBINE.Rdata\")\nhead(GASTURBINE, n=5)\ntail(GASTURBINE, n=5)\n# stepwise/stepwise regression\nGASTURBINE <- na.omit(GASTURBINE) # step() requires removal of missing data before if any\nfull.gas.model <- lm(HEATRATE ~ ENGINE + SHAFTS + RPM + CPRATIO + INLETTEMP + EXHTEMP + AIRFLOW + POWER + LHV + ISOWORK,  data = GASTURBINE)\ngas.model <- step(full.gas.model, direction=\"both\")\nsummary(gas.model)\n```\n\n(b) Stepwise with backward elimination\n```{r}\ngas.model <- step(full.gas.model, direction=\"backward\")\nsummary(gas.model)\n```\n\n(c) All-possible-regressions-selection / \"best subset\"\n```{r}\nlibrary(leaps)\ny = c(\"HEATRATE\")\nx = c(\"SHAFTS\",\"RPM\",\"CPRATIO\",\"INLETTEMP\",\"EXHTEMP\",\"AIRFLOW\",\"POWER\",\"HEATRATE\", \"LHV\",\"ISOWORK\")\nbest.model <- leaps(x = GASTURBINE[,x], y=GASTURBINE[,y], names=x, nbest=3, method=\"adjr2\")\nbest.model$which # shows the T or F of variable inclusion in the model\napply(best.model$which, 1, which) # consise print-out of each best model\nbest.model$adjr2\n```\n\n(d) Which Independent variable are consistency selected out of the previous results:    \nStepwise: EXHTEMP, POWER, ISOWORK     \nBackwards elimination: ENGINE, SHAFTS, RPM, CPRATIO, INLETTEMP, AIRFLOW, LHV      \nbest subset: POWER, LHV, CPRATIO, RPM\n\n(e) I would use the previous results from each of the model selection\n  techniques to select the most important variables (that also exist\n  most frequently in across each model) based on each final outcome of\n  the elimination techniques and take into account their p-value and AIC\n  score.\n\n\n\\pagebreak\npage 377 #7.2\n\n(a) The problems that exist when multicollinearity exist are: high\n  correlations among independent variables increase the likelihood of\n  rounding errors in the calculations of the beta estimate from the\n  underlying matrix operation from the computers difficulty in\n  inverting the information matrix. Multicollinearity can also cause\n  misleading and confusing results of the signs of the parameter\n  estimates than what is expected.\n  \n(b) You can detect multicollinearity with several methods:  \n\n* Calculate the coefficient of correlation between each pair of independent\n  variables in the model; if one or more of the r values is close\n  to 1 or -1, the variables are highly correlated and a severe\n  milticollinearity problem may be present.  \n  \n*  Non-significant t-test's for the individual beta parameters when\n  the F-test for overall model adequacy is significant and estimates\n  with opposite signs from what is expected.  \n  \n*  Calculation of the variance inflation factor (vif) for the individual\n  factors.\n\n(c) Solutions to address multicollinearity are:\n\n* Drop one or more of the correlated independent variables\n\n*  if the correlate variables are kept in the model, avoid making\n  inferences about the individual parameters.\n  \n* Use a designed experiment\n\n\\pagebreak\n\npage 377 #7.5\n\n(a) No extreme multicollinearity exists according in the correlation matrix\n\n(b) According to the multiple regression output on page 190, There is evidence for multicollinearity: Significant F-test, with p-value < 0.001 when there are multiple non-significant (high p-values) independent x variables. \n\n\\pagebreak\n\npage 378 #7.10\n\n(a) fit a first-order model to the data, E(y) = 5.71059 + 0.62597 LIVEWT\n\n```{r}\nload(\"~/Desktop/depaul/CSC423/rdata/R/Exercises&Examples/STEERS.Rdata\")\nhead(STEERS)\nDRESSWT = STEERS$DRESSWT\nLIVEWT = STEERS$LIVEWT\nsteer.model <- lm(DRESSWT ~ LIVEWT, data=STEERS)\nsummary(steer.model)\n```\n\n(b) 95% prediction interval for the dressed weight of a 300 pound steer\n```{r}\npredict(steer.model, newdata=data.frame(DRESSWT = 300), interval=\"prediction\", level=0.95)\n```\n\n(c) Yes, The interval is tight as being a prediction interval and with the single x-variable of 300 the interval is a close approximation as to have minimal complaints from customers.\n\n\n\n\\pagebreak\npage 381 #7.20\n\n(a) Scatter plot of points. There seems to be a negative linear relationship between X and Y possibly even curvilinear sloping up.\n\n```{r}\nload(\"~/Desktop/depaul/CSC423/rdata/R/Exercises&Examples/EX7_20.Rdata\")\nEX7_20\nplot(EX7_20$X, EX7_20$Y, main=\"EX7_20 Scatterplot\", pch=\"+\", col=\"darkblue\")\n```\n\n(b)  calculate ln x and ln y, then plot the log transformed data on another scatter plot. This plot shows similarly to the other plot a negative linear relationship and possibly a curvilinear sloping down.\n```{r}\nEX7_20$Xln <- log(EX7_20$X)\nEX7_20$Yln <- log(EX7_20$Y)\nplot(EX7_20$Xln, EX7_20$Yln, main=\"EX7_20 Scatterplot: Log Transformation\", pch=\"+\", col=\"orange\")\n```\n\n(c) Fit transformed data to model equation. The  F-statistic: 180.7 on 1 and 9 DF,  p-value: 2.911e-07 is high with a significant p-value above the alpha = 0.05.\n```{r}\nYln = EX7_20$Yln\nXln = EX7_20$Xln\nlogs.on.logs <- lm(Yln ~ Xln, data=EX7_20)\nsummary(logs.on.logs)\n```\n\n(d) prediction of y, when x=30\n```{r}\nexp(predict(logs.on.logs, newdata = data.frame(Xln = 30), interval=\"prediction\", level=0.95))\n```\n\n\\pagebreak\npage 381 #7.21\n\n(a) Coefficient of correlation between y and x1. Since the value is so low (0.0025) there seems to be no evidence of a linear relationship between y and x1\n```{r}\nload(\"~/Desktop/depaul/CSC423/rdata/R/Exercises&Examples/HAMILTON.Rdata\")\nhead(HAMILTON)\ncor(HAMILTON$X1, HAMILTON$Y)\n```\n\n(b) Coefficient of correlation between y and x2. Since the value is so low (0.43) there seems to be no evidence of a linear relationship between y and x2\n```{r}\ncor(HAMILTON$X2, HAMILTON$Y)\n```\n\n(c) Based on the previous results, I do not think that the model will be a useful predictor of sale price\n\n(d) Fit the model: y = -45.154136 + 3.097008 X1 + 1.031859 X2   \nThe R^2 value & adjusted R^2 is very high (0.9998) and the F-statistic has a significant p-value so that would imply that the model disagrees with the findings in the previous answer in part c.\n```{r}\nham <- lm(Y ~ X1 + X2, data=HAMILTON)\nsummary(ham)\n```\n\n(e) Coefficient of correlation between x1 and x2. The result is close to -1 implying there is high correlation between x1 and x2.\n```{r}\ncor(HAMILTON$X1, HAMILTON$X2)\n```\n\n(f) I would not recommend this strategy for this example. The confidence for E(y) and prediction intervals for y generally remain unaffected as long as the values of the independent variables used to predict y follow the same pattern of multicollinearity exhibited in the sample data. The x1 and x2 variables may not be very redundant.\n\n\\pagebreak\npage 381 #7.22\n\n(a) Independent variables that are moderately or highly correlated:   \n(5) Foreign Status with (3) Race  -0.515\n(9) Years in graduate program with (7) Year GRE was taken -0.602\n\n(b) If those independent variables are left in the model, you could observe unreliable beta estimates and incorrect signs.\n\n\n",
    "created" : 1449854185014.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1078090865",
    "id" : "CF8D82B8",
    "lastKnownWriteTime" : 1445960175,
    "path" : "~/Desktop/depaul/CSC423/chp6-7-HW-jasmine_dumas.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}